# Asymptotic Notation
https://www.youtube.com/watch?v=iOq5kSKqeR4

- We can't directly compare speeds of algorithms in minutes/seconds, as it is dependent on the inputs, hardware/software en of the algorithm.
    - Because of this, programmers came up with concepts of measuring the Asymptotic Complexity of a program, using a notation called Big(O) notation:
    
    - Its a way of analyzing how fast a program's run time increases asymptotically.
    > As the size of the inputs increases towards infinity, how does the runtime of the program grow?





